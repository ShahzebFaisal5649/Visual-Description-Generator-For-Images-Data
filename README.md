# Image Captioning with LSTM

This project implements an image captioning model using a Long Short-Term Memory (LSTM) neural network. The model generates descriptive captions for images by combining features extracted from the images with text sequences.

## Overview

Image captioning is a challenging task in artificial intelligence that involves generating human-like descriptions for images automatically. This project aims to create a model that can accurately describe the content of images in natural language.

## Features

- Merges image features with corresponding captions for training.
- Utilizes LSTM architecture to process image features and text sequences.
- Employs BLEU score for evaluation, a common metric in natural language processing tasks.
- Provides examples of both good and bad captions generated by the model for qualitative evaluation.

## Requirements

- Python 3.x
- TensorFlow 2.x
- Keras
- NumPy
- NLTK (Natural Language Toolkit)
- Matplotlib

## Usage

1. **Data Preparation**: Prepare your dataset containing images and their corresponding captions. Ensure that the captions are preprocessed and tokenized.

2. **Training**: Train the LSTM model using the provided script. You may need to adjust hyperparameters such as batch size, learning rate, and number of epochs based on your dataset and computational resources.

3. **Evaluation**: Evaluate the trained model using the provided evaluation script. Calculate the BLEU score to measure the quality of generated captions compared to ground truth captions.

4. **Example Generation**: Use the model to generate captions for sample images from your dataset. Visualize the generated captions along with the corresponding images for qualitative assessment.

## Files

- `data_preparation.py`: Prepares the dataset by merging image features with captions and tokenizing the captions.
- `lstm_model.py`: Defines the LSTM model architecture and preprocessing generator for training.
- `train_model.py`: Trains the LSTM model using the prepared dataset and monitors training progress using Tensorboard.
- `evaluate_model.py`: Evaluates the trained model using BLEU score and provides examples of generated captions.
- `README.md`: Documentation providing an overview of the project, usage instructions, and other relevant information.

## Results

- The model achieved a BLEU score of 0.875, indicating high similarity between generated captions and ground truth captions.
- Examples of both good and bad captions are provided to illustrate the model's performance qualitatively.

## Conclusion

This project demonstrates the effectiveness of using LSTM neural networks for image captioning tasks. By combining image features with text sequences, the model can generate descriptive captions that closely resemble human-written captions. Further improvements can be made by fine-tuning hyperparameters and experimenting with different model architectures.

